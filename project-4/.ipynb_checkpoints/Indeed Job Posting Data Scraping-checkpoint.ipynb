{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_lang= ['r','python','java','c++','ruby','perl','matlab','javascript','scala']\n",
    "analysis_tool = ['excel','tableau','d3.js''sas','spss','d3'] \n",
    "hadoop = ['hadoop','mapreduce','spark','pig','hive','spark','oozie','zookeeper','flume']     \n",
    "database = ['sql','nosql','hbase','cassandra','mongodb']\n",
    "\n",
    "### processing job summary, only extract english word\n",
    "def text_cleaner(website):\n",
    "    '''\n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "\n",
    "    soup_obj = BeautifulSoup(urlopen(website).read().decode('utf-8'), 'html.parser')\n",
    "   \n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    text = soup_obj.get_text() # Get the text from this\n",
    "    lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "          \n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    \n",
    "    def chunk_space(chunk):\n",
    "        chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "        return chunk_out  \n",
    "        \n",
    "    \n",
    "    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "    text_ori = text.decode(\"utf-8\")\n",
    "    text = re.sub(\"[^a-zA-Z.+3]\",\" \", text_ori)\n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    " \n",
    "    try:   \n",
    "        stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed\n",
    "    skill=[]\n",
    "    programming='None'\n",
    "    analysis='None'\n",
    "    hadoop='None'\n",
    "    database='None'\n",
    "    if len(set(text) & set (prog_lang)):\n",
    "        programming=\"_\".join(list(set(text) & set (prog_lang)))\n",
    "        skill.append('programming')\n",
    "    elif len(set(text) & set (analysis_tool)):\n",
    "        analysis=\"_\".join(list(set(text) & set (analysis_tool)))\n",
    "        skill.append('analysis')\n",
    "    elif len(set(text) & set (hadoop)):\n",
    "        hadoop=\"_\".join(list(set(text) & set (hadoop)))\n",
    "        skill.append('hadoop')\n",
    "    elif len(set(text) & set (database)):\n",
    "        database=\"_\".join(list(set(text) & set (database)))\n",
    "        skill.append('database')\n",
    "    if len(skill)>0:\n",
    "        skill=\"_\".join(skill)\n",
    "    else:\n",
    "        skill='None'\n",
    "    # or not on the website)\n",
    "    return (text_ori,skill,programming,analysis,hadoop,database) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Data Scientist - Seattle, WA - Indeed.com Find JobsCompany ReviewsFind SalariesFind ResumesEmployers / Post JobUpload your resumeSign inHomewhatjob title, keywords, or companywherecity, state, or zip codeFind JobsAdvanced Job SearchData ScientistIndeed355 reviews-Seattle, WAThis job posting is no longer available on Indeed.Related searches:Data Scientist jobs in Seattle, WAIndeed jobs in Seattle, WAIndeed355 reviewsRead what people are saying about working here.How A Data Scientist Works As a Data Scientist at Indeed your role is to follow the data. Analyze, visualize, and model job search related data. You will build and implement machine learning models to make timely decisions. You will have access to unparalleled resources within Indeed to grow and develop both personally and professionally. We are looking for a mixture between a statistician, scientist, machine learning expert and engineer: someone who has passion for building and improving Internet-scale products informed by data. The ideal candidate understands human behavior and knows what to look for in the data. Responsibilities: Drive the roadmap proactively by identifying opportunities in the data Evaluate the big picture and solve business problems rather than focusing on metrics alone Create great visualizations We're looking for someone who: Can do small data modeling work: R, Julia, Octave, Python Can do big data modeling work: Hadoop, Hive, Impala, Pig, Scala, Scalding, Scoobi, Spark, Shark Can fish for data: SQL, Unix skills, Perl, python pandas, or other languages Has hands on coding skills in one of : SQL, bash , perl, python or other languages to do your own data analysis Can communicate concisely and persuasively with engineers and product marketers Requirements: Ph.D. or M.S. in a natural science where you ran experiments and analysed data statistically Academic experience in one (or more) of these fields: physics, bio-statistics, astrophysics, computational chemistry, cognitive or neuro sciences, behavioral economics, econometrics, finance, mathematics, computer science Work experience analyzing lots of data, for example in biotechnology, astrophysics or particle physics experiments, quantitative finance or high frequency trading, “war on terror” intelligence analytics, or consumer Internet Benefits & Perks: Competitive salaries and bonus programs Medical, Dental and Vision coverage Commuter benefits Wellness initiatives (incentive programs, fitness classes) Retirement plan with company match (401K) Catered meals, endless snacks and drinks Monthly happy hours Casual dress code Employee development opportunities Flexible Work Arrangements How to Apply: We value attention to detail. Applications that fail to follow these simple steps will not be considered. Create an Indeed Resume on Indeed.com and please highlight in it as much as possible your data science experience that matches the requirements for the position. Please apply with your Indeed Resume. P lease visit How Indeed Works to learn more: http://www.indeed.jobsIndeed - 3 years ago - save job - report job - original jobOther jobs you may likeResearch Data Scientist (Geospatial) - IntermediatTechnology PartnersSeattle, WATechnology Partners-11 hours agoSenior Data ScientistTeslaSeattle, WAEasily apply21 hours agoData Scientist : 18-04654Akraya Inc.Bellevue, WAAKRAYA INC.-20 hours agoSee more recommended jobsData Scientist jobs in Seattle, WAJobs at Indeed in Seattle, WAData Scientist salaries in Seattle, WACompany InfoFollowGet job updates from IndeedIndeed355 reviewsIndeed is the #1 job site worldwide*, with over 200 million unique visitors per month from more than 60 countries in 28 languages. Si...Let employers find youThousands of employers search for candidates on IndeedUpload your resumeAboutHelp Center© 2018 IndeedCookies, Privacy and TermsLet Employers Find YouUpload Your Resume \",\n",
       " 'programming',\n",
       " 'python_scala_perl_r',\n",
       " 'None',\n",
       " 'None',\n",
       " 'None')"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing = text_cleaner(\"http://www.indeed.com/viewjob?jk=5505e59f8e5a32a4&q=%22data+scientist%22&tk=19ftfgsmj19ti0l3&from=web&advn=1855944161169178&sjdu=QwrRXKrqZ3CNX5W-O9jEvWC1RT2wMYkGnZrqGdrncbKqQ7uwTLXzT1_ME9WQ4M-7om7mrHAlvyJT8cA_14IV5w&pub=pub-indeed\")\n",
    "testing[:20] # Just show the first 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Jobs 1 to 10 of 282 jobs found\n",
      "\n",
      "\n",
      "        Jobs 1 to 10 of 126 jobs found\n",
      "\n",
      "\n",
      "        Jobs 1 to 10 of 28 jobs found\n",
      "\n",
      "\n",
      "        Jobs 1 to 10 of 563 jobs found\n",
      "\n",
      "\n",
      "        Jobs 1 to 1 of 1 jobs found\n",
      "\n",
      "Done with collecting job postings in indeed.com!\n",
      "There were 499 jobs successfully found.\n"
     ]
    }
   ],
   "source": [
    "#URL =\"https://www.indeed.com.sg/jobs?q=data+scientist+%241000&l=Singapore&start=10\"\n",
    "\n",
    "job_ids = pd.DataFrame()\n",
    "#['fulltime','permanent','contract','intership','temporary']\n",
    "\n",
    "\n",
    "for job_type in ['fulltime','permanent','contract','intership','temporary']:\n",
    "        result_list = []   \n",
    "        URL =\"https://www.indeed.com.sg/jobs?q=data+scientist&l=Singapore&jt={}\".format(job_type)\n",
    "        #conducting a request of the stated URL above:\n",
    "        soup_for_count = BeautifulSoup(urlopen(URL).read(), 'html.parser')\n",
    "        results_number = soup_for_count.find(\"div\", attrs = {\"id\": \"searchCount\"}).text\n",
    "        print(results_number, 'jobs found\\n') # Display how many jobs were found\n",
    "    \n",
    "        number_of_results = int(results_number.split(sep = ' ')[-1].replace(',', ''))\n",
    "        i = int(number_of_results/100)\n",
    "        for page_number in range(i + 1):\n",
    "            URL_for_results = \"https://www.indeed.com.sg/jobs?q=data+scientist&l=Singapore&jt={}&limit=1000&start={}\".format(job_type,str(100 * page_number))\n",
    "            soup_for_results = BeautifulSoup(urlopen(URL_for_results).read(), 'html.parser')\n",
    "            results = soup_for_results.find_all('div', attrs={'data-tn-component': 'organicJob'})\n",
    "            # Extract the ID for each job listing\n",
    "            location='None'\n",
    "            salaries='None'\n",
    "            summary='None'\n",
    "            \n",
    "            for x in results:\n",
    "                job_id = x.find('h2', attrs={\"class\": \"jobtitle\"})['id']\n",
    "                job_title = x.find('a', attrs={'data-tn-element': \"jobTitle\"}).text.strip().capitalize()\n",
    "                company = x.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
    "                if len(company) > 0:\n",
    "                    for b in company:\n",
    "                        job_company=b.text.strip()\n",
    "                else:\n",
    "                    sec_try = x.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
    "                    for span in sec_try:\n",
    "                        job_company=span.text.strip()\n",
    "\n",
    "                spans = x.findAll(\"span\", attrs={\"class\": \"location\"})\n",
    "                for span in spans:\n",
    "                    location=span.text\n",
    "\n",
    "                for div in x.find_all(name=\"td\", attrs={\"class\":\"snip\"}):\n",
    "                    try:\n",
    "                        div_two = div.find(name=\"span\", attrs={\"class\":\"no-wrap\"})\n",
    "                        salaries=div_two.text.strip()\n",
    "                    except:\n",
    "                        salaries=\"None\"\n",
    "                spans = x.findAll(\"span\", attrs={\"class\": \"summary\"})\n",
    "                for span in spans:\n",
    "                    summary=span.text.strip()\n",
    "\n",
    "                job_link =\"https://www.indeed.com.sg\"+x.find('h2', attrs={\"class\": \"jobtitle\"}).find('a')['href']\n",
    "                #driver = webdriver.Chrome(executable_path=chromedriver)\n",
    "                #driver.get(x.find('h2', attrs={\"class\": \"jobtitle\"}).find('a')['href']).click()\n",
    "                job_descriptions=[]\n",
    "                job_desc='None'\n",
    "                try:\n",
    "                    job_desc,skill,programming,analysis,hadoop,database = text_cleaner(job_link)\n",
    "                except:\n",
    "                    next\n",
    "                result_list.append([job_id, job_title,job_company,location,salaries,job_type,summary,job_link,job_desc,skill,programming,analysis,hadoop,database])\n",
    "\n",
    "            # Add the job ID numbers\n",
    "            job_ids = job_ids.append(result_list)\n",
    "\n",
    "        # Remove re-posted jobs\n",
    "        job_ids.drop_duplicates(inplace = True)\n",
    "\n",
    "job_ids.columns=[\"job_id\", \"job_title\",\"job_company\",\"location\",\"salaries\",\"job_type\",\"summary\",\"job_link\",\"job_desc\",\"skill\",\"programming\",\"analysis\",\"hadoop\",\"database\"]\n",
    "\n",
    "\n",
    "print('Done with collecting job postings in indeed.com!')  \n",
    "print('There were', len(job_ids), 'jobs successfully found.')\n",
    "job_ids.to_csv(\"./{}.csv\".format('scientist'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only 499 data scientist jobs were found in indeed.com. Next,decided to scrap job with 'data analyst' keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Jobs 1 to 10 of 453 jobs found\n",
      "\n",
      "\n",
      "        Jobs 1 to 10 of 259 jobs found\n",
      "\n",
      "\n",
      "        Jobs 1 to 10 of 1,978 jobs found\n",
      "\n",
      "\n",
      "        Jobs 1 to 10 of 57 jobs found\n",
      "\n",
      "Done with collecting job postings in indeed.com!\n",
      "There were 1056 jobs successfully found.\n"
     ]
    }
   ],
   "source": [
    "#URL =\"https://www.indeed.com.sg/jobs?q=data+scientist+%241000&l=Singapore&start=10\"\n",
    "\n",
    "job_ids = pd.DataFrame()\n",
    "#['fulltime','permanent','contract','intership','temporary']\n",
    "for job_type in ['fulltime','permanent','contract','intership','temporary']:\n",
    "        result_list = []   \n",
    "        URL =\"https://www.indeed.com.sg/jobs?q=data+analyst&l=Singapore&jt={}\".format(job_type)\n",
    "        #conducting a request of the stated URL above:\n",
    "        soup_for_count = BeautifulSoup(urlopen(URL).read(), 'html.parser')\n",
    "        results_number = soup_for_count.find(\"div\", attrs = {\"id\": \"searchCount\"}).text\n",
    "        print(results_number, 'jobs found\\n') # Display how many jobs were found\n",
    "    \n",
    "        number_of_results = int(results_number.split(sep = ' ')[-1].replace(',', ''))\n",
    "        i = int(number_of_results/100)\n",
    "        for page_number in range(i + 1):\n",
    "            URL_for_results = \"https://www.indeed.com.sg/jobs?q=data+analyst&l=Singapore&jt={}&limit=1000&start={}\".format(job_type,str(100 * page_number))\n",
    "            soup_for_results = BeautifulSoup(urlopen(URL_for_results).read(), 'html.parser')\n",
    "            results = soup_for_results.find_all('div', attrs={'data-tn-component': 'organicJob'})\n",
    "            # Extract the ID for each job listing\n",
    "            location='None'\n",
    "            salaries='None'\n",
    "            summary='None'\n",
    "            \n",
    "            for x in results:\n",
    "                job_id = x.find('h2', attrs={\"class\": \"jobtitle\"})['id']\n",
    "                job_title = x.find('a', attrs={'data-tn-element': \"jobTitle\"}).text.strip().capitalize()\n",
    "                company = x.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
    "                if len(company) > 0:\n",
    "                    for b in company:\n",
    "                        job_company=b.text.strip()\n",
    "                else:\n",
    "                    sec_try = x.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
    "                    for span in sec_try:\n",
    "                        job_company=span.text.strip()\n",
    "\n",
    "                spans = x.findAll(\"span\", attrs={\"class\": \"location\"})\n",
    "                for span in spans:\n",
    "                    location=span.text\n",
    "\n",
    "                for div in x.find_all(name=\"td\", attrs={\"class\":\"snip\"}):\n",
    "                    try:\n",
    "                        div_two = div.find(name=\"span\", attrs={\"class\":\"no-wrap\"})\n",
    "                        salaries=div_two.text.strip()\n",
    "                    except:\n",
    "                        salaries=\"None\"\n",
    "                spans = x.findAll(\"span\", attrs={\"class\": \"summary\"})\n",
    "                for span in spans:\n",
    "                    summary=span.text.strip()\n",
    "\n",
    "                job_link =\"https://www.indeed.com.sg\"+x.find('h2', attrs={\"class\": \"jobtitle\"}).find('a')['href']\n",
    "                #print(job_link)\n",
    "                #driver = webdriver.Chrome(executable_path=chromedriver)\n",
    "                #driver.get(x.find('h2', attrs={\"class\": \"jobtitle\"}).find('a')['href']).click()\n",
    "                job_descriptions=[]\n",
    "                job_desc='None'\n",
    "                try:\n",
    "                    job_desc,skill,programming,analysis,hadoop,database = text_cleaner(job_link)\n",
    "                except:\n",
    "                    next\n",
    "                result_list.append([job_id, job_title,job_company,location,salaries,job_type,summary,job_link,job_desc,skill,programming,analysis,hadoop,database])\n",
    "\n",
    "            # Add the job ID numbers\n",
    "            job_ids = job_ids.append(result_list)\n",
    "\n",
    "        # Remove re-posted jobs\n",
    "        job_ids.drop_duplicates(inplace = True)\n",
    "\n",
    "job_ids.columns=[\"job_id\", \"job_title\",\"job_company\",\"location\",\"salaries\",\"job_type\",\"summary\",\"job_link\",\"job_desc\",\"skill\",\"programming\",\"analysis\",\"hadoop\",\"database\"]\n",
    "\n",
    "\n",
    "print('Done with collecting job postings in indeed.com!')  \n",
    "print('There were', len(job_ids), 'jobs successfully found.')\n",
    "job_ids.to_csv(\"./{}.csv\".format('analyst'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### concatenate both table\n",
    "\n",
    "df_a=pd.read_csv('./analyst.csv')\n",
    "df_s=pd.read_csv('./scientist.csv')\n",
    "\n",
    "merge_df=pd.concat([df_a,df_s])\n",
    "merge_df.shape\n",
    "merge_df.to_csv('./analyst_scientist.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
